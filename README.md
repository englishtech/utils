**/async_wikipedia_pagedata_parser.py**

Асинхронный парсер русской Википедии: по списку URL вытягивает из инфобоксов страниц даты рождения/смерти, продолжительность жизни (в днях), имя и профессию и складывает всё в SQLite.
Читает plaintext-файл со ссылками (по 1 в строке) (имя файла вводится в терминале).
Работает асинхронно, может парсить файл с ~100000 ссылок (протестировано). Но оптимально разбивать tasks на чанки по ~50-80 ссылок - при большем числе одновременных запросов Википедия может отключить соединение.

- За ~4-5 сек. обрабатывает пачку (≈15× быстрее синхронного цикла) 

- Результат – БД `my_database.db`. Никаких внешних модулей кроме `aiohttp, bs4, requests`  

**/async_wikipedia_search_parser.py**

Универсальный асинхронный парсер русской Википедии. Подходит для быстрого сбора базы вики-статей под любую тематику — от персоналий до технических терминов.
По любому запросу в терминале скрипт сам пролистает всю поисковую выдачу на странице поиска (https://ru.wikipedia.org/w/index.php?search=&title=...)
*В Википедии есть ограничение выдачи результатов до 10000 элементов.

- Настраивается пагинация, если результаты поиска не помещаются на одну страницу (макс. 5000 элементов/стр.)

- Сохраняет ссылки построчно в текстовый файл, название - "транслитерированный_запрос.txt"

- Работает без API (только aiohttp, bs4, requests), устойчив к ошибкам, показывает прогресс и время выполнения. 
