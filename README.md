# utils
My personal Python-based utilities:

**/async_wikipedia_pagedata_parser.py**

Парсит страницы Википедии. Ссылки на них загружаются из файла .txt (вводится в терминале).
Работает асинхронно, может парсить файл с ~100000 ссылок (протестировано). 
Но оптимально разбивать tasks на чанки по ~50-80 ссылок - при большем числе одновременных запросов Википедия может отключить соединение.
На странице Википедии парсит данные из инфобокса. См. код.
На выходе парсера - словарь, он сохраняется в список словарей по каждой url.
Этот список сохраняется поэлементно в базу данных sqlite3

**/async_wikipedia_search_parser.py**

Парсит все ссылки на странице поиска Википедии (https://ru.wikipedia.org/w/index.php?search=&title=...) по строке запроса, вводимого в терминале.
Настраивается пагинация, если результаты поиска не помещаются на одну страницу (макс. 5000 элементов/стр.)
В Википедии есть ограничение на 10000 элементов в поисковой выдаче.
Сохраняет ссылки построчно в текстовый файл, название - "транслитерированный_запрос.txt"
